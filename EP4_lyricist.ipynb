{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "secondary-australian",
   "metadata": {},
   "source": [
    "# 4. 작사가 인공지능 만들기\n",
    "## 1. 데이터읽어오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "healthy-barrel",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "데이터 크기: 187088\n",
      "Examples:\n",
      " ['At first I was afraid', 'I was petrified', 'I kept thinking I could never live without you', 'By my side But then I spent so many nights', \"Just thinking how you've done me wrong\", 'I grew strong', \"I learned how to get along And so you're back\", 'From outer space', 'I just walked in to find you', 'Here without that look upon your face I should have changed that fucking lock']\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import os, re\n",
    "import tensorflow as tf\n",
    "\n",
    "txt_file_path = os.getenv('HOME')+'/aiffel/lyricist/data/lyrics/*'\n",
    "txt_list = glob.glob(txt_file_path)\n",
    "\n",
    "# 여러개의 txt 파일을 모두 읽어서 raw_corpus 에 담습니다.\n",
    "raw_corpus = []\n",
    "for txt_file in txt_list:\n",
    "    with open(txt_file, \"r\") as f:\n",
    "        raw = f.read().splitlines()\n",
    "        raw_corpus.extend(raw)\n",
    "\n",
    "print(\"데이터 크기:\", len(raw_corpus))\n",
    "print(\"Examples:\\n\", raw_corpus[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "labeled-instrument",
   "metadata": {},
   "source": [
    "## 2. 데이터 정제\n",
    "### 2-1. 정규표현식을 이용한 corpus생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "comic-construction",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 정규표현식으로 문장 정제하는 함수 \n",
    "def preprocess_sentence(sentence):\n",
    "    #     1. 소문자로 바꾸고, 양쪽 공백을 지웁니다\n",
    "    #     2. 특수문자 양쪽에 공백을 넣고\n",
    "    #     3. 여러개의 공백은 하나의 공백으로 바꿉니다\n",
    "    #     4. a-zA-Z?.!,¿가 아닌 모든 문자를 하나의 공백으로 바꿉니다\n",
    "    #     5. 다시 양쪽 공백을 지웁니다\n",
    "    #     6. 문장 시작에는 <start>, 끝에는 <end>를 추가합니다\n",
    "    sentence = sentence.lower().strip() # 1\n",
    "    sentence = re.sub(r\"([?.!,¿])\", r\" \\1 \", sentence) # 2\n",
    "    sentence = re.sub(r'[\" \"]+', \" \", sentence) # 3\n",
    "    sentence = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", sentence) # 4\n",
    "    sentence = sentence.strip() # 5\n",
    "    sentence = '<start> ' + sentence + ' <end>' # 6\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "compact-hostel",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<start> at first i was afraid <end>', '<start> i was petrified <end>', '<start> i kept thinking i could never live without you <end>', '<start> by my side but then i spent so many nights <end>', '<start> just thinking how you ve done me wrong <end>', '<start> i grew strong <end>', '<start> i learned how to get along and so you re back <end>', '<start> from outer space <end>', '<start> i just walked in to find you <end>', '<start> i would have made you leave your key <end>']\n",
      "총문장수: 156013\n"
     ]
    }
   ],
   "source": [
    "# corpus에 정제된 문장 모음.\n",
    "corpus = []\n",
    "length = 15\n",
    "for sentence in raw_corpus:\n",
    "    # 우리가 원하지 않는 문장은 건너뜁니다\n",
    "    if len(sentence) == 0: continue\n",
    "    if sentence[-1] == \":\": continue\n",
    "    # 정제를 하고 담아주세요\n",
    "    preprocessed_sentence = preprocess_sentence(sentence)\n",
    "    if len(list(preprocessed_sentence.split()))> length: \n",
    "        continue\n",
    "    else: \n",
    "        corpus.append(preprocessed_sentence)\n",
    "# 정제된 결과를 10개만 확인\n",
    "print(corpus[:10])\n",
    "print(\"총문장수:\", len(corpus))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lesbian-grenada",
   "metadata": {},
   "source": [
    "### 2-2. Tokenizer 패키지로 corpus를 텐서로 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "steady-biodiversity",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   2   70  248 ...    0    0    0]\n",
      " [   2    4   53 ...    0    0    0]\n",
      " [   2    4 1077 ...    0    0    0]\n",
      " ...\n",
      " [   2    8    4 ...    0    0    0]\n",
      " [   2   44   17 ...    0    0    0]\n",
      " [   2    6  172 ...    0    0    0]] <keras_preprocessing.text.Tokenizer object at 0x7f49932f9d10>\n"
     ]
    }
   ],
   "source": [
    "# tokenize() 함수로 데이터를 Tensor로 변환.\n",
    "def tokenize(corpus):\n",
    "    # 텐서플로우에서 제공패키지\n",
    "    tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
    "        num_words=12000, # 전체 단어의 개수\n",
    "        filters=' ',     # 별도로 전처리 로직을 추가할 수 있음. \n",
    "        oov_token=\"<unk>\"# out-of-vocabulary\n",
    "    )\n",
    "    tokenizer.fit_on_texts(corpus) # corpus를 이용해 tokenizer 내부의 단어장을 완성\n",
    "    tensor = tokenizer.texts_to_sequences(corpus) # 준비한 tokenizer를 이용해 corpus를 Tensor로 변환합니다\n",
    "    # 입력 데이터의 시퀀스 길이를 일정하게 맞춰줍니다(최대길이maxlen 설정가능)\n",
    "    # 문장 앞에 패딩을 붙여 길이를 맞추고 싶다면 padding='pre'를 사용합니다\n",
    "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post')  \n",
    "    print(tensor,tokenizer)\n",
    "    return tensor, tokenizer\n",
    "\n",
    "tensor, tokenizer = tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "scientific-devices",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   2   70  248    4   53  708    3    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [   2    4   53 6263    3    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [   2    4 1077  531    4  104   80  192  257    7    3    0    0    0\n",
      "     0]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(156013, 15)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 변환된 텐서 확인\n",
    "print(tensor[:3])\n",
    "tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "closed-advance",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 : <unk>\n",
      "2 : <start>\n",
      "3 : <end>\n",
      "4 : i\n",
      "5 : ,\n",
      "6 : the\n",
      "7 : you\n",
      "8 : and\n",
      "9 : a\n",
      "10 : to\n"
     ]
    }
   ],
   "source": [
    "# tokenizer에 구축된 단어 사전의 인덱스\n",
    "for idx in tokenizer.index_word:\n",
    "    print(idx, \":\", tokenizer.index_word[idx])\n",
    "    if idx >= 10: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "productive-matter",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  2  70 248   4  53 708   3   0   0   0   0   0   0   0]\n",
      "[ 70 248   4  53 708   3   0   0   0   0   0   0   0   0]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "156013"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 소스문장/타겟문장을 만든다. \n",
    "src_input = tensor[:, :-1]  # 마지막 토큰은 <end>가 아니라 <pad>일 가능성이 높습니다.\n",
    "tgt_input = tensor[:, 1:] # tensor에서 <start>를 잘라내서 타겟 문장을 생성합니다.\n",
    "print(src_input[0]) #소스문장 예시 \n",
    "print(tgt_input[0]) #타겟문장 예시\n",
    "len(src_input) #소스문장수"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "selective-mineral",
   "metadata": {},
   "source": [
    "## 3. 평가 데이터셋 분리 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "saved-pixel",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source Train: (124810, 14)\n",
      "Target Train: (124810, 14)\n"
     ]
    }
   ],
   "source": [
    "# train,validation set분리 \n",
    "from sklearn.model_selection import train_test_split\n",
    "enc_train, enc_val, dec_train, dec_val = train_test_split(src_input, tgt_input, test_size=0.20, random_state=42)\n",
    "print(\"Source Train:\", enc_train.shape)\n",
    "print(\"Target Train:\", dec_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "endless-record",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset shapes: ((256, 14), (256, 14)), types: (tf.int32, tf.int32)>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 파라미터 설정\n",
    "BUFFER_SIZE = len(src_input)\n",
    "BATCH_SIZE = 256\n",
    "steps_per_epoch = len(src_input) // BATCH_SIZE\n",
    "\n",
    "\n",
    "# 준비한 데이터 소스로부터 데이터셋을 만듭니다\n",
    "dataset = tf.data.Dataset.from_tensor_slices((enc_train, dec_train))\n",
    "dataset = dataset.shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
    "dataset "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "brutal-antigua",
   "metadata": {},
   "source": [
    "## 4. 모델 설계 및 훈련"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "creative-brush",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12001\n",
      "Epoch 1/10\n",
      "487/487 [==============================] - 159s 316ms/step - loss: 4.0004\n",
      "Epoch 2/10\n",
      "487/487 [==============================] - 156s 319ms/step - loss: 3.0194\n",
      "Epoch 3/10\n",
      "487/487 [==============================] - 155s 319ms/step - loss: 2.8365\n",
      "Epoch 4/10\n",
      "487/487 [==============================] - 155s 319ms/step - loss: 2.6955\n",
      "Epoch 5/10\n",
      "487/487 [==============================] - 155s 318ms/step - loss: 2.5799\n",
      "Epoch 6/10\n",
      "487/487 [==============================] - 155s 318ms/step - loss: 2.4628\n",
      "Epoch 7/10\n",
      "487/487 [==============================] - 155s 318ms/step - loss: 2.3665\n",
      "Epoch 8/10\n",
      "487/487 [==============================] - 155s 317ms/step - loss: 2.2736\n",
      "Epoch 9/10\n",
      "487/487 [==============================] - 155s 318ms/step - loss: 2.1840\n",
      "Epoch 10/10\n",
      "487/487 [==============================] - 155s 318ms/step - loss: 2.0967\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f49801967d0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TextGenerator클래스 \n",
    "class TextGenerator(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_size, hidden_size):\n",
    "        super().__init__()    \n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_size)\n",
    "        self.rnn_1 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n",
    "        self.rnn_2 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n",
    "        self.linear = tf.keras.layers.Dense(vocab_size)\n",
    "        \n",
    "    def call(self, x):\n",
    "        out = self.embedding(x)\n",
    "        out = self.rnn_1(out)\n",
    "        out = self.rnn_2(out)\n",
    "        out = self.linear(out)   \n",
    "        return out\n",
    "\n",
    "#세팅\n",
    "embedding_size = 526\n",
    "hidden_size = 1024 # 1024\n",
    "VOCAB_SIZE = tokenizer.num_words + 1   \n",
    "print(VOCAB_SIZE)\n",
    "model = TextGenerator(tokenizer.num_words + 1, embedding_size , hidden_size)\n",
    "\n",
    "# 실행\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
    "model.compile(loss=loss, optimizer=optimizer)\n",
    "model.fit(dataset, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "exciting-somerset",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문장 생성 함수 \n",
    "def generate_text(model, tokenizer, init_sentence=\"<start>\", max_len=20):\n",
    "    # 테스트를 위해서 입력받은 init_sentence도 텐서로 변환합니다\n",
    "    test_input = tokenizer.texts_to_sequences([init_sentence])\n",
    "    test_tensor = tf.convert_to_tensor(test_input, dtype=tf.int64)\n",
    "    end_token = tokenizer.word_index[\"<end>\"]\n",
    "    # 단어 하나씩 예측해 문장을 만듭니다\n",
    "    #    1. 입력받은 문장의 텐서를 입력합니다\n",
    "    #    2. 예측된 값 중 가장 높은 확률인 word index를 뽑아냅니다\n",
    "    #    3. 2에서 예측된 word index를 문장 뒤에 붙입니다\n",
    "    #    4. 모델이 <end>를 예측했거나, max_len에 도달했다면 문장 생성을 마칩니다\n",
    "    while True:\n",
    "        # 1\n",
    "        predict = model(test_tensor) \n",
    "        # 2\n",
    "        predict_word = tf.argmax(tf.nn.softmax(predict, axis=-1), axis=-1)[:, -1] \n",
    "        # 3 \n",
    "        test_tensor = tf.concat([test_tensor, tf.expand_dims(predict_word, axis=0)], axis=-1)\n",
    "        # 4\n",
    "        if predict_word.numpy()[0] == end_token: break\n",
    "        if test_tensor.shape[1] >= max_len: break\n",
    "    generated = \"\"\n",
    "    # tokenizer를 이용해 word index를 단어로 하나씩 변환합니다 \n",
    "    for word_index in test_tensor[0].numpy():\n",
    "        generated += tokenizer.index_word[word_index] + \" \"\n",
    "    return generated\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ruled-commitment",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> i love you , i m a liar <end> '"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 문장 생성 함수 실행하여 모델에게 작문 시켜보기 \n",
    "generate_text(model, tokenizer, init_sentence=\"<start> i love\", max_len=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "random-blood",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> i believe in god , <end> '"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, tokenizer, init_sentence=\"<start> I believe\", max_len=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dress-cooler",
   "metadata": {},
   "source": [
    "## 프로젝트 정리\n",
    "- text를 컴퓨터가 알아들을 수 있는 형식으로 변경하는 과정(text->숫자)과 단어 예측하는 일련의 과정을 경험해보니 많이 배울 수 있었음. \n",
    "- 정규표현식이 복잡하긴 했지만, 사람이 일일이 수고를 하는 것에 비하면 편리한 방법이므로 잘 배워놔야겠다고 생각함. \n",
    "- 자원의 효율성을 고려하여 문장 길이를 15단어로 제한하여 학습을 진행했고, 손실지표는 지속적으로 줄어드는 모습을 보였음. \n",
    "- 모델에 다양한 문장을 적용하여 예측을 하였는데, 사람이 쓸 법한 결과를 도출하여 학습이 잘된 것 같았음. \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
